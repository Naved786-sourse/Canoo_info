# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f2RszF0wa1iG0jf5sQnViynBwyALoQ2c

#Document Load()
"""

pip install langchain

pip install unstructured libmagic python-magic python-magic-bin

pip install unstructured

pip install faiss-cpu

!pip install openai

pip install sentence-transformers

pip install tiktoken

!pip install faiss-cpu
!pip install sentence-transformers

import os
import pickle
import time
import langchain
from langchain import OpenAI
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredURLLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from sentence_transformers import SentenceTransformer

loaders = UnstructuredURLLoader(urls=[
    "https://www.canoo.com/about/",
    "https://en.wikipedia.org/wiki/Canoo",
    "https://www.canoo.com/canoo/",
    "https://simplywall.st/stocks/us/automobiles/nasdaq-goev/canoo",
    "https://insights.greyb.com/canoo-patents/",
    "https://investors.canoo.com/company-information/management-team"
])

# Now you can use the url_loader object to process the URLs

data = loaders.load()
len(data)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# As data is of type documents we can directly use split_documents over split_text in order to get the chunks.
docs = text_splitter.split_documents(data)

len(docs)

docs[0]

len(chunks)

os.environ['OPENAI_API_KEY'] = "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
llm = OpenAI(temperature=0.9, max_tokens=500)

# Create the embeddings of the chunks using openAIEmbeddings
embeddings = OpenAIEmbeddings()

# Pass the documents and embeddings inorder to create FAISS vector index
vectorindex_openai = FAISS.from_documents(docs, embeddings)

chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorindex_openai.as_retriever())
chain

import pandas as pd
queries = [
    "What industry does Canoo operate in?",
    "What is Canoo's size?",
    "What is Canoo's growth rate?",
    "What are the trends in Canoo's industry?",
    "Identify Canoo's main competitors.",
    "What is the market share of Canoo's competitors?",
    "What products or services do Canoo's competitors offer?",
    "What pricing strategies do Canoo's competitors use?"
]

# Perform the queries and collect responses
responses = {}
for query in queries:
    response = chain({"question": query}, return_only_outputs=True)
    responses[query] = response

# Organize responses into a DataFrame
df = pd.DataFrame(responses.items(), columns=['Query', 'Response'])

# Save responses to CSV
df.to_csv('canoo_info.csv', index=False)

